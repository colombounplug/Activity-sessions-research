In 2012, we had an idea for a measurement strategy that would bring insight into understand a online community.  While studying the nature of participation in Wikipedia, the open, collaborative encyclopedia, we found ourselves increasingly curious about the amount of time that volunteer contributors invested into the encyclopedia's construction.  While past work had relied on counting the number of contributions made by a user\footnote{"Wikipedian is first to hit 1 million edits" \url{http://www.dailydot.com/news/wikipedian-first-1-million-edits}} as a measure of investment, we felt that the amount of time a user invested into editing might more accurately measure investment.  This supposition was inspired by past work attempting to estimate the amount of time invested into Wikipedia as a whole\cite{clay shirky's 100 million hours}.

The measurement strategy we came up with is based on the clustering of Wikipedia editors' activities into edit sessions with the assumption that the duration of an edit session would represent a lower bound of the amount of time invested into Wikipedia contributions\cite{geiger13using}.  While we found the notion of work sessions to be intuitive from our ethnographic work in Wikipedia, we did not find a consensus in the literature for how to identify sessions from timestamped user activities.  So, we looked to the data itself for insight on what might be a reasonable approach splitting users' editing activity into sessions. The regularities we found in inter-activity time amazed us with their intuitiveness and the simplicity of session demarcation implied. It is that work that lead us to look for such regularities in other systems and to write this paper to share our results.

We are not the first to try our hands at trying to identify a reasonable way to measure user session behavior in human-computer interaction.  User sessions have been used extensively to generate metrics for understanding the performance of information resources\cite{govseva2006empirical} -- especially in the domain of search[cite cite cite] and content personalisation\cite{spiliopoulou2003framework,gomory1999analysis}. Despite this interest in understand the nature and manifestation of user sessions, no clear consensus has emerged.  In fact, some work has gone as far as arguing that sessions don't actually exist as a useful divide for user activity\cite{jones2008beyond} and that the strategy of choosing a global inactivity threshold is arbitrary\cite{montgomery2001identifying}.

In this paper, we will propose and demonstrate a strategy for identifying user sessions from log data and demonstrate that the strategy works consistently across many different types systems and user activity.  We'll start by summarizing previous work trying to make sense of user session behavior from log data.  We'll also discuss theoretical arguments about how intuitive user behavior (e.g. tasks and sessions) ought to manifest in the data. Next, we'll discuss a generalized version of the session threshold identification strategy we developed in \cite{geiger13using} and present strategies for fitting this strategy to new data.  Then, we'll introduce 6 different systems from which we have extracted 12 different logged user action types for analysis and comparison. Finally, we'll conclude with discussions of the regularities and irregularities between datasets and what that might imply for both our understanding of human behavior and the measurement of it.
